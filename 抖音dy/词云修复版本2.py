import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from wordcloud import WordCloud
import jieba
from collections import Counter
import json
from datetime import datetime
import os
import warnings
import matplotlib as mpl


# å½»åº•è§£å†³ä¸­æ–‡æ˜¾ç¤ºé—®é¢˜
def setup_chinese_font_comprehensive():
    """å…¨é¢è®¾ç½®ä¸­æ–‡å­—ä½“"""
    try:
        # æ–¹æ³•1: å°è¯•macOSç³»ç»Ÿå­—ä½“
        mac_fonts = [
            'PingFang SC',
            'Hiragino Sans GB',
            'Heiti SC',
            'Arial Unicode MS',
            'STHeiti',
            'Microsoft YaHei'
        ]

        # æ£€æŸ¥ç³»ç»Ÿå­—ä½“
        available_fonts = []
        for font in mac_fonts:
            try:
                test_font = mpl.font_manager.FontProperties(family=font)
                if mpl.font_manager.findfont(test_font) != test_font.get_name():
                    available_fonts.append(font)
            except:
                continue

        if available_fonts:
            plt.rcParams['font.sans-serif'] = available_fonts + ['DejaVu Sans']
            print(f"âœ… ä½¿ç”¨ç³»ç»Ÿå­—ä½“: {available_fonts[0]}")
        else:
            # æ–¹æ³•2: ä½¿ç”¨matplotlibçš„é»˜è®¤å­—ä½“ï¼Œä½†ç”¨è‹±æ–‡æ ‡ç­¾
            plt.rcParams['font.sans-serif'] = ['DejaVu Sans']
            print("âš ï¸  ä½¿ç”¨è‹±æ–‡æ ‡ç­¾ï¼ˆç³»ç»Ÿç¼ºå°‘ä¸­æ–‡å­—ä½“ï¼‰")

        plt.rcParams['axes.unicode_minus'] = False

    except Exception as e:
        print(f"å­—ä½“è®¾ç½®å¤±è´¥: {e}")
        plt.rcParams['font.sans-serif'] = ['DejaVu Sans']


# åˆå§‹åŒ–å­—ä½“è®¾ç½®
setup_chinese_font_comprehensive()

# å¿½ç•¥è­¦å‘Š
warnings.filterwarnings('ignore')

# # æå–è¯„è®ºæ•°æ®

# 1. å°†æ•°æ®ä¿å­˜ä¸ºdata.jsonæ–‡ä»¶
# 2. ä½¿ç”¨ä»¥ä¸‹ä»£ç åŠ è½½
import json
with open('data.json', 'r', encoding='utf-8') as f:
    comments_data = json.load(f)  # ä¼šè‡ªåŠ¨è½¬æ¢false/null


def safe_extract_comments(data):
    """å®‰å…¨æå–è¯„è®ºä¿¡æ¯"""
    comments_list = []

    if not data or 'comments' not in data:
        print("é”™è¯¯ï¼šæ•°æ®æ ¼å¼ä¸æ­£ç¡®")
        return pd.DataFrame()

    comments = data['comments']
    print(f"æ‰¾åˆ° {len(comments)} æ¡è¯„è®º")

    for i, comment in enumerate(comments):
        try:
            if 'create_time' not in comment or 'text' not in comment:
                continue

            comment_info = {
                'cid': comment.get('cid', f'unknown_{i}'),
                'text': str(comment.get('text', '')),
                'create_time': comment.get('create_time'),
                'digg_count': comment.get('digg_count', 0),
                'user_nickname': comment.get('user', {}).get('nickname', 'æœªçŸ¥ç”¨æˆ·'),
                'ip_label': comment.get('ip_label', 'æœªçŸ¥åœ°åŒº'),
                'reply_comment_total': comment.get('reply_comment_total', 0)
            }

            try:
                comment_info['create_time'] = datetime.fromtimestamp(comment_info['create_time'])
            except:
                comment_info['create_time'] = datetime.now()

            comments_list.append(comment_info)

        except Exception as e:
            print(f"å¤„ç†ç¬¬{i + 1}æ¡è¯„è®ºæ—¶å‡ºé”™: {e}")
            continue

    return pd.DataFrame(comments_list)


def create_chinese_visualizations(df):
    """åˆ›å»ºä¸­æ–‡å¯è§†åŒ–å›¾è¡¨"""
    if df.empty:
        print("æ²¡æœ‰æ•°æ®å¯åˆ†æ")
        return

    # åˆ›å»ºå›¾å½¢
    fig, axes = plt.subplots(2, 2, figsize=(16, 12))

    # 1. ç‚¹èµæ•°åˆ†å¸ƒ - ä½¿ç”¨ä¸­æ–‡æ ‡é¢˜
    axes[0, 0].hist(df['digg_count'], bins=20, alpha=0.7, color='#1f77b4', edgecolor='black')
    axes[0, 0].set_title('è¯„è®ºç‚¹èµæ•°åˆ†å¸ƒ', fontsize=14, fontweight='bold')
    axes[0, 0].set_xlabel('ç‚¹èµæ•°')
    axes[0, 0].set_ylabel('è¯„è®ºæ•°é‡')
    axes[0, 0].grid(True, alpha=0.3)

    # 2. åœ°åŸŸåˆ†å¸ƒ - ç›´æ¥ä½¿ç”¨ä¸­æ–‡
    region_counts = df['ip_label'].value_counts().head(8)
    bars = axes[0, 1].bar(range(len(region_counts)), region_counts.values,
                          color=['#ff7f0e', '#2ca02c', '#d62728', '#9467bd',
                                 '#8c564b', '#e377c2', '#7f7f7f', '#bcbd22'])
    axes[0, 1].set_title('è¯„è®ºåœ°åŸŸåˆ†å¸ƒTOP8', fontsize=14, fontweight='bold')
    axes[0, 1].set_ylabel('è¯„è®ºæ•°é‡')
    axes[0, 1].set_xticks(range(len(region_counts)))
    axes[0, 1].set_xticklabels(region_counts.index, rotation=45, ha='right')

    # åœ¨æŸ±çŠ¶å›¾ä¸Šæ˜¾ç¤ºæ•°å€¼
    for bar, count in zip(bars, region_counts.values):
        height = bar.get_height()
        axes[0, 1].text(bar.get_x() + bar.get_width() / 2., height,
                        f'{count}', ha='center', va='bottom')

    # 3. æ–‡æœ¬é•¿åº¦åˆ†æ
    df['text_length'] = df['text'].str.len()
    axes[1, 0].hist(df['text_length'], bins=15, alpha=0.7, color='#2ca02c', edgecolor='black')
    axes[1, 0].set_title('è¯„è®ºæ–‡æœ¬é•¿åº¦åˆ†å¸ƒ', fontsize=14, fontweight='bold')
    axes[1, 0].set_xlabel('æ–‡æœ¬é•¿åº¦ï¼ˆå­—ç¬¦æ•°ï¼‰')
    axes[1, 0].set_ylabel('è¯„è®ºæ•°é‡')
    axes[1, 0].grid(True, alpha=0.3)

    # 4. é«˜ç‚¹èµè¯„è®º - ä½¿ç”¨ç¼–å·ä»£æ›¿é•¿æ–‡æœ¬
    top_comments = df.nlargest(5, 'digg_count')[['digg_count', 'ip_label']].copy()
    y_pos = range(len(top_comments))
    bars = axes[1, 1].barh(y_pos, top_comments['digg_count'], color='#ff7f0e')
    axes[1, 1].set_yticks(y_pos)
    # ä½¿ç”¨ç®€çŸ­æ ‡ç­¾
    labels = [f'è¯„è®º{i + 1}[{top_comments.iloc[i]["ip_label"]}]' for i in range(len(top_comments))]
    axes[1, 1].set_yticklabels(labels)
    axes[1, 1].set_title('é«˜ç‚¹èµè¯„è®ºTOP5', fontsize=14, fontweight='bold')
    axes[1, 1].set_xlabel('ç‚¹èµæ•°')

    # åœ¨æ¡å½¢å›¾ä¸Šæ˜¾ç¤ºæ•°å€¼
    for bar, count in zip(bars, top_comments['digg_count']):
        width = bar.get_width()
        axes[1, 1].text(width, bar.get_y() + bar.get_height() / 2.,
                        f'{count}', ha='left', va='center', fontweight='bold')

    plt.tight_layout()
    # åœ¨plt.show()ä¹‹å‰æ·»åŠ 
    # ä¿å­˜è¯äº‘å›¾ç‰‡
    plt.savefig('analysis_chart.png', dpi=300, bbox_inches='tight', facecolor='white')
    plt.show()

    # å•ç‹¬æ˜¾ç¤ºé«˜ç‚¹èµè¯„è®ºå†…å®¹
    print("\nğŸ“‹ é«˜ç‚¹èµè¯„è®ºè¯¦æƒ…:")
    top_comments_detail = df.nlargest(5, 'digg_count')[['text', 'digg_count', 'ip_label']]
    for i, (idx, row) in enumerate(top_comments_detail.iterrows()):
        print(f"{i + 1}. [{row['ip_label']}] ğŸ‘{row['digg_count']}")
        print(f"   {row['text']}")
        print()


def create_advanced_wordcloud(df):
    """åˆ›å»ºé«˜çº§è¯äº‘ï¼ˆè§£å†³ä¸­æ–‡é—®é¢˜ï¼‰"""
    try:
        # æ–‡æœ¬å¤„ç†
        all_text = ' '.join(df['text'].astype(str))
        words = jieba.cut(all_text)

        # æ‰©å±•åœç”¨è¯åˆ—è¡¨
        stop_words = {
            'çš„', 'äº†', 'åœ¨', 'æ˜¯', 'æˆ‘', 'æœ‰', 'å’Œ', 'å°±', 'ä¸', 'äºº', 'éƒ½', 'ä¸€',
            'ä¸€ä¸ª', 'ä¸Š', 'ä¹Ÿ', 'å¾ˆ', 'åˆ°', 'è¯´', 'è¦', 'å»', 'ä½ ', 'ä¼š', 'ç€',
            'æ²¡æœ‰', 'çœ‹', 'å¥½', 'è‡ªå·±', 'è¿™', 'é‚£', 'å¥¹', 'ä»–', 'å®ƒ', 'æˆ‘ä»¬', 'ä½ ä»¬',
            'ä»–ä»¬', 'è¿™ä¸ª', 'é‚£ä¸ª', 'è¿™äº›', 'é‚£äº›', 'è¿™æ ·', 'é‚£æ ·', 'è¿™é‡Œ', 'é‚£é‡Œ'
        }

        filtered_words = [word for word in words if len(word) > 1 and word not in stop_words]
        word_freq = Counter(filtered_words)

        print("ğŸ¨ ç”Ÿæˆé«˜çº§è¯äº‘...")

        # macOS å­—ä½“è·¯å¾„
        font_paths = [
            '/System/Library/Fonts/PingFang.ttc',  # è‹¹æ–¹å­—ä½“
            '/System/Library/Fonts/STHeiti Light.ttc',  # åæ–‡é»‘ä½“
            '/System/Library/Fonts/STHeiti Medium.ttc',
            '/Library/Fonts/Arial Unicode.ttf',
            None  # ä½¿ç”¨é»˜è®¤å­—ä½“
        ]

        wordcloud = None
        used_font = "é»˜è®¤å­—ä½“"

        for font_path in font_paths:
            try:
                if font_path and os.path.exists(font_path):
                    wordcloud = WordCloud(
                        font_path=font_path,
                        width=1000,
                        height=600,
                        background_color='white',
                        max_words=150,
                        colormap='viridis',
                        relative_scaling=0.5,
                        stopwords=set(),  # æˆ‘ä»¬å·²ç»æ‰‹åŠ¨è¿‡æ»¤äº†
                        collocations=False
                    ).generate_from_frequencies(word_freq)
                    used_font = os.path.basename(font_path)
                    print(f"âœ… ä½¿ç”¨å­—ä½“: {used_font}")
                    break
                elif font_path is None:
                    wordcloud = WordCloud(
                        width=1000,
                        height=600,
                        background_color='white',
                        max_words=150,
                        colormap='viridis'
                    ).generate_from_frequencies(word_freq)
                    used_font = "ç³»ç»Ÿé»˜è®¤"
                    print("â„¹ï¸  ä½¿ç”¨é»˜è®¤å­—ä½“")
                    break
            except Exception as e:
                continue

        if wordcloud:
            # åˆ›å»ºæ›´å¤§çš„å›¾å½¢
            plt.figure(figsize=(16, 9))
            plt.imshow(wordcloud, interpolation='bilinear')
            plt.axis('off')
            plt.title('æŠ–éŸ³è¯„è®ºå…³é”®è¯è¯äº‘åˆ†æ', fontsize=20, fontweight='bold', pad=20)
            plt.tight_layout()
            plt.show()

            # æ˜¾ç¤ºè¯é¢‘ç»Ÿè®¡
            print("\nğŸ“Š è¯é¢‘ç»Ÿè®¡TOP15:")
            for i, (word, count) in enumerate(word_freq.most_common(15)):
                print(f"   {i + 1:2d}. {word}: {count}æ¬¡")

        else:
            print("âŒ è¯äº‘ç”Ÿæˆå¤±è´¥ï¼Œè·³è¿‡...")

    except Exception as e:
        print(f"âŒ ç”Ÿæˆè¯äº‘æ—¶å‡ºé”™: {e}")


def create_sentiment_analysis(df):
    """ç®€å•æƒ…æ„Ÿåˆ†æ"""
    print("\nğŸ˜Š æƒ…æ„Ÿå€¾å‘åˆ†æ:")

    # ç®€å•å…³é”®è¯æƒ…æ„Ÿåˆ†æ
    positive_words = ['æ”¯æŒ', 'å¥½', 'å–œæ¬¢', 'èµ', 'æ£’', 'ä¼˜ç§€', 'æ„Ÿè°¢', 'çˆ±', 'ç¾ä¸½', 'å¼€å¿ƒ', 'åŠ æ²¹']
    negative_words = ['åå¯¹', 'ä¸å¥½', 'è®¨åŒ', 'åƒåœ¾', 'æ¶å¿ƒ', 'æ¨', 'ä¸‘é™‹', 'ä¼¤å¿ƒ', 'æµè¡€', 'ç‰ºç‰²', 'éª‚']

    sentiments = []
    for text in df['text']:
        pos_count = sum(1 for word in positive_words if word in text)
        neg_count = sum(1 for word in negative_words if word in text)

        if pos_count > neg_count:
            sentiments.append('ç§¯æ')
        elif neg_count > pos_count:
            sentiments.append('æ¶ˆæ')
        else:
            sentiments.append('ä¸­æ€§')

    df['sentiment'] = sentiments

    # æƒ…æ„Ÿåˆ†å¸ƒ
    sentiment_counts = df['sentiment'].value_counts()
    print("æƒ…æ„Ÿåˆ†å¸ƒ:")
    for sentiment, count in sentiment_counts.items():
        percentage = (count / len(df)) * 100
        print(f"  {sentiment}: {count}æ¡ ({percentage:.1f}%)")

    # æƒ…æ„Ÿä¸ç‚¹èµçš„å…³ç³»
    print("\næƒ…æ„Ÿä¸ç‚¹èµæ•°å…³ç³»:")
    sentiment_stats = df.groupby('sentiment')['digg_count'].agg(['mean', 'max', 'count'])
    print(sentiment_stats.round(2))


def save_enhanced_report(df):
    """ä¿å­˜å¢å¼ºç‰ˆåˆ†ææŠ¥å‘Š"""
    try:
        with open('æŠ–éŸ³è¯„è®ºæ·±åº¦åˆ†ææŠ¥å‘Š.txt', 'w', encoding='utf-8') as f:
            f.write("æŠ–éŸ³è¯„è®ºæ·±åº¦åˆ†ææŠ¥å‘Š\n")
            f.write("=" * 60 + "\n\n")

            f.write(f"åˆ†ææ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
            f.write(f"æ€»è¯„è®ºæ•°: {len(df)}\n")
            f.write(f"æ€»ç‚¹èµæ•°: {df['digg_count'].sum()}\n")
            f.write(f"å¹³å‡ç‚¹èµæ•°: {df['digg_count'].mean():.2f}\n")
            f.write(f"è¯„è®ºæ—¶é—´èŒƒå›´: {df['create_time'].min()} åˆ° {df['create_time'].max()}\n\n")

            f.write("ğŸ“Š åŸºæœ¬ç»Ÿè®¡åˆ†æ\n")
            f.write("-" * 40 + "\n")
            f.write(f"æœ€é•¿è¯„è®º: {df['text'].str.len().max()} å­—ç¬¦\n")
            f.write(f"æœ€çŸ­è¯„è®º: {df['text'].str.len().min()} å­—ç¬¦\n")
            f.write(f"å¹³å‡è¯„è®ºé•¿åº¦: {df['text'].str.len().mean():.1f} å­—ç¬¦\n\n")

            f.write("ğŸŒ åœ°åŸŸåˆ†å¸ƒåˆ†æ\n")
            f.write("-" * 40 + "\n")
            for region, count in df['ip_label'].value_counts().items():
                percentage = (count / len(df)) * 100
                f.write(f"{region}: {count}æ¡ ({percentage:.1f}%)\n")

            f.write("\nâ­ é«˜ç‚¹èµè¯„è®ºåˆ†æ\n")
            f.write("-" * 40 + "\n")
            top_comments = df.nlargest(10, 'digg_count')
            for i, (idx, row) in enumerate(top_comments.iterrows()):
                f.write(f"\n{i + 1}. [{row['ip_label']}] ğŸ‘{row['digg_count']}\n")
                f.write(f"   å†…å®¹: {row['text']}\n")

            # é«˜é¢‘è¯æ±‡åˆ†æ
            all_text = ' '.join(df['text'].astype(str))
            words = jieba.cut(all_text)
            stop_words = {'çš„', 'äº†', 'åœ¨', 'æ˜¯', 'æˆ‘', 'æœ‰', 'å’Œ', 'å°±', 'ä¸', 'äºº', 'éƒ½', 'ä¸€'}
            filtered_words = [word for word in words if len(word) > 1 and word not in stop_words]
            word_freq = Counter(filtered_words)

            f.write("\nğŸ”¤ é«˜é¢‘è¯æ±‡åˆ†æ\n")
            f.write("-" * 40 + "\n")
            for i, (word, count) in enumerate(word_freq.most_common(25)):
                f.write(f"{i + 1:2d}. {word}: {count}æ¬¡\n")

            # ç”¨æˆ·åˆ†æ
            f.write("\nğŸ‘¤ ç”¨æˆ·åˆ†æ\n")
            f.write("-" * 40 + "\n")
            user_stats = df['user_nickname'].value_counts()
            f.write(f"æ€»ç”¨æˆ·æ•°: {len(user_stats)}\n")
            f.write("æ´»è·ƒç”¨æˆ·TOP5:\n")
            for user, count in user_stats.head().items():
                f.write(f"  {user}: {count}æ¡è¯„è®º\n")

        print("ğŸ“„ æ·±åº¦åˆ†ææŠ¥å‘Šå·²ä¿å­˜åˆ°: æŠ–éŸ³è¯„è®ºæ·±åº¦åˆ†ææŠ¥å‘Š.txt")
    except Exception as e:
        print(f"ä¿å­˜æŠ¥å‘Šæ—¶å‡ºé”™: {e}")


# ä¸»ç¨‹åº
def main():
    print("ğŸš€ å¼€å§‹æ·±åº¦åˆ†ææŠ–éŸ³è¯„è®ºæ•°æ®...")

    # æå–æ•°æ®
    df = safe_extract_comments(comments_data)

    if df.empty:
        print("âŒ æ²¡æœ‰æå–åˆ°æœ‰æ•ˆè¯„è®ºæ•°æ®")
        return

    print(f"âœ… æ•°æ®æå–æˆåŠŸï¼å…± {len(df)} æ¡è¯„è®º")

    # ç”Ÿæˆä¸­æ–‡å¯è§†åŒ–å›¾è¡¨
    print("\nğŸ“Š ç”Ÿæˆä¸­æ–‡å¯è§†åŒ–å›¾è¡¨...")
    create_chinese_visualizations(df)

    # ç”Ÿæˆé«˜çº§è¯äº‘
    print("\nğŸ¨ ç”Ÿæˆé«˜çº§è¯äº‘...")
    create_advanced_wordcloud(df)

    # æƒ…æ„Ÿåˆ†æ
    create_sentiment_analysis(df)

    # ä¿å­˜å¢å¼ºç‰ˆæŠ¥å‘Š
    save_enhanced_report(df)

    print("\nğŸ‰ æ·±åº¦åˆ†æå®Œæˆï¼")
    print("ğŸ’¡ æç¤º: å¦‚æœå›¾è¡¨ä¸­æ–‡ä»æ˜¾ç¤ºå¼‚å¸¸ï¼Œè¯·æ£€æŸ¥ç³»ç»Ÿä¸­æ–‡å­—ä½“å®‰è£…æƒ…å†µ")


if __name__ == "__main__":
    main()